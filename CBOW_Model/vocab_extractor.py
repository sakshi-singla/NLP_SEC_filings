import os
os.environ["OMP_NUM_THREADS"] = "6" 
os.environ["OPENBLAS_NUM_THREADS"] = "6" 
os.environ["MKL_NUM_THREADS"] = "6" 
os.environ["VECLIB_MAXIMUM_THREADS"] = "6" 
os.environ["NUMEXPR_NUM_THREADS"] = "6" 

import os
import torch
import numpy as np
from nltk.tokenize import RegexpTokenizer
import pickle
import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import string
import nltk
from nltk.stem.porter import *
import codecs
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

nltk.download('punkt')

torch.manual_seed(1)

CONTEXT_SIZE = 4
glove_path = 'glove.6B'
printable = set(string.printable)

def save_glove_vocab_to_pickle():
    words = []
    vectors = []
    idx = 0
    word2idx = {}
    # vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.50.dat', mode='w')

    with open(f'{glove_path}/glove.6B.50d.txt', 'rb') as f:
        for l in f:
            line = l.decode().split()
            word = line[0]
            words.append(word)
            word2idx[word] = idx
            idx += 1
            vect = np.array(line[1:]).astype(np.float)
            vectors.append(vect)

    # vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=f'{glove_path}/6B.50.dat', mode='w')
    # vectors.flush()
    pickle.dump(vectors, open(f'{glove_path}/6B.50_vectors.pkl', 'wb'))
    pickle.dump(words, open(f'{glove_path}/6B.50_words.pkl', 'wb'))
    pickle.dump(word2idx, open(f'{glove_path}/6B.50_idx.pkl', 'wb'))


# def remove_non_ascii(words):
#     """Remove non-ASCII characters from list of tokenized words"""
#     import unicodedata
#     new_words = []
#     for word in words:
#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
#         new_words.append(new_word)
#     return new_words

def read_data(file_path):

    tokenizer = RegexpTokenizer(r'\w+')
    text = codecs.open(file_path, 'r', encoding='utf-8', errors='ignore').read()
    valid_characters = string.printable
    text = ''.join(i for i in text if i in valid_characters)
    text = text.lower()
    text = re.sub('[' + string.punctuation + '0-9\\r\\t\\n\Â®]', ' ', text)
    tokens = nltk.word_tokenize(text)
    tokens = [w for w in tokens if len(w) > 2]  # ignore a, an, to, at, be, ...
    # lemmatizer = WordNetLemmatizer()
    # tokens = [lemmatizer.lemmatize(w) for w in tokens]
    goodWords = [w for w in tokens if w not in ENGLISH_STOP_WORDS]
    # goodWords2 = remove_non_ascii(goodWords)
    return (goodWords)


def get_whole_text_as_List(filesFolder):

    fnames = os.listdir(filesFolder)
    complete_test_sentence = []
    for file in fnames:
        if file.endswith('.txt'):
            print("tokenizing file:", file)
            test_sentence = read_data(filesFolder + file)
            complete_test_sentence = complete_test_sentence+test_sentence
    return complete_test_sentence


